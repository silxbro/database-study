# 7.1 | 통계정보와 비용 계산 원리

SQL 옵티마이저에 대한 설명으로 본서를 시작했는데, 마지막 장에서 다시 설명하려는 이유는 좀 더 구체적인 이미지를 심어주기 위해서다.
SQL 튜닝뿐 아니라 데이터베이스를 안정적으로 관리하기 위해서도 옵티마이저에 대한 정확한 이해가 필수인데, 1장에서 짧게 설명한 개념 수준의 이해만으로는 부족하다.
복잡한 비용 계산 원리까지 학습할 필요는 없지만, 옵티마이저가 통계정보를 어떻게 활용하는지는 간략하게라도 알아야 한다.

선택도와 카디널리티에 이어 통계정보에 구체적으로 어떤 항목들이 있는지 살펴보자. 마지막으로, 옵티마이저가 통계정보를 이용해 비용을 계산하는 원리를 간략하게 살펴보자.

<br/>

## (1) 선택도와 카디널리티
선택도(Selectivity)란, 전체 레코드 중에서 조건절에 의해 선택되는 레코드 비율을 말한다.
가장 단순한 '=' 조건으로 검색할 때의 선택도만 살펴보면, 컬럼 값 종류 개수(Number of Distinct Values, 이하 'NDV')를 이용해 아래와 같이 구한다.
```
선택도 = 1 / NDV
```

카디널리티(Cardinality)란, 전체 레코드 중에서 조건절에 의해 선택되는 레코드 개수이며, 아래 공식으로 구한다.
```
카디널리티 = 총 로우 수 ✕ 선택도 = 총 로우 수 / NDV
```

예를 들어, 상품분류 컬럼에 '가전', '의류', '식음료', '생활용품' 네 개의 값이 있을 때, 아래 조건절에 대한 선택도는 25%(=1/4)이다. 만약 전체 레코드가 10만 건이면, 카디널리티는 2만 5천이다.
```
WHERE 상품분류 = '가전'
```

옵티마이저는 이렇게 카디널리티를 구하고, 그만큼의 데이터를 액세스하는 데 드는 비용을 계산해서 테이블 액세스 방식, 조인 순서, 조인 방식 등을 결정한다.
공식을 통해 알 수 있듯, 비용을 계산하는 출발점은 선택도다. 선택도를 잘못 계산하면, 카디널리티와 비용도 잘못 계산하고, 결과적으로 비효율적인 액세스 방식과 조인 방식을 선택하게 된다.
선택도를 계산할 때 NDV를 사용하므로 통계정보 수집 과정에서 이 값을 정확히 구하는 것이 매우 중요하다. 통계정보 수집주기, 샘플링 비율 등을 잘 결정해야 하는 이유다.

<br/>

## (2) 통계정보
통계정보에는 오브젝트 통계와 시스템 통계가 있다. 오브젝트 통계는 다시 테이블 통계, 인덱스 통계, 컬럼 통계(히스토그램 포함)로 나뉜다.

### [1] 테이블 통계
테이블 통계를 수집하는 명령어는 다음과 같다.
```
begin
  dbms_stats.gather_table_stats('scott', 'emp');
end;
/
```
수집된 테이블 통계정보는 아래와 같이 조회할 수도 있고, all_tab_statistics 뷰에서도 같은 정보를 확인할 수 있다.
```
select num_rows, blocks, avg_row_len, sample_size, last_analyzed
from   all_tables
where  owner = 'SCOTT'
and    table_name = 'EMP';
```

아래 표는 주요 테이블 통계항목에 대한 설명이다.

|통계항목|설명|
|:---|:---|
|NUM_ROWS|테이블에 저장된 총 레코드 개수|
|BLOCKS|테이블 블록 수 = '사용된' 익스텐트(데이터가 한 건이라도 입력된 적이 있는 모든 익스텐트)에 속한 총 블록 수<br/>- 테이블에 '할당된' 총 블록 수는 dba_segments 또는 user_segments 뷰에서 확인 가능|
|AVG_ROW_LEN|레코드당 평균 길이(Bytes)|
|SAMPLE_SIZE|샘플링한 레코드 수|
|LAST_ANALYZED|통계정보 수집일시|

### [2] 인덱스 통계
인덱스 통계를 수집하는 명령어는 다음과 같다.
```
-- 인덱스 통계만 수집
begin
  dbms_stats.gather_index_stats ( ownname => 'scott', indname = 'emp_x01');
end;
/

-- 테이블 통계를 수집하면서 인덱스 통계도 같이 수집
begin
  dbms_stats.gather_table_stats ('scott', 'emp', cascade=>true);
end;
/
```

수집된 인덱스 통계정보는 아래와 같이 조회할 수 있으며, all_ind_statistics 뷰에서도 같은 정보를 확인할 수 있다.
```
select blevel, leaf_blocks, num_rows, distinct_keys
     , avg_leaf_blocks_per_key, avg_data_blocks_per_key, clustering_factor
from   all_indexes
where  owner = 'SCOTT'
and    table_name = 'EMP'
and    index_name = 'EMP_X01' ;
```

아래 표는 주요 인덱스 통계항목에 대한 설명이다.

|통계항목|설명|용도|
|:---|:---|:---|
|BLEVEL|브랜치 레벨의 약자.<br/>인덱스 루트에서 리프 블록에 도달하기 직전까지 읽게 되는 블록 수|인덱스 수직적 탐색 비용 계산|
|LEAF_BLOCKS|인덱스 리프 블록 총 개수|인덱스 수평적 탐색 비용 계산|
|NUM_ROWS|인덱스에 저장된 레코드 개수|인덱스 수평적 탐색 비용 계산|
|DISTINCT_KEYS|인덱스 키값의 조합으로 만들어지는 값의 종류 개수.<br/>예를 들어, C1+C2로 구성한 인덱스에서 C1 컬럼에 3개, C2 컬럼에 4개<br/>값이 있으면 최대 12개 값의 종류가 만들어질텐데, 인덱스에 저장된 데이터<br/>기준으로 실제 입력된 값의 종류 개수를 구해 놓은 수치. 인덱스 키값을 모두<br/>'=' 조건으로 조회할 때의 선택도(Selectivity)를 계산하는 데 사용|인덱스 수평적 탐색 비용 계산|
|AVG_LEAF_<br/>BLOCKS_PER_KEY|인덱스 키값을 모두 '=' 조건으로 조회할 때 읽게 될 리프 블록 개수|인덱스 수평적 탐색 비용 계산|
|AVG_DATA_<br/>BLOCKS_PER_KEY|인덱스 키값을 모두 '=' 조건으로 조회할 때 읽게 될 테이블 블록 개수|테이블 액세스 비용 계산|
|CLUSTERING_FACTOR|인덱스 키값 기준으로 테이블 데이터가 모여 있는 정도.<br/>인덱스 전체 레코드를 스캔하면서 테이블 레코드를 찾아갈 때 읽게 될<br/>테이블 블록 개수를 미리 계산해 놓은 수치|테이블 액세스 비용 계산|

### [3] 컬럼 통계
컬럼 통계는 테이블 통계 수집할 때 함께 수집된다. 수집된 컬럼 통계정보는 아래와 같이 조회할 수 있다. all_tab_col_statistics 뷰에서도 같은 정보를 확인할 수 있다.
```
select num_distinct, density, avg_col_len, low_value, high_value, num_nulls
     , last_analyzed, sample_size
from   all_tab_columns
where  owner = 'SCOTT'
and    table_name = 'EMP'
and    column_name = 'DEPTNO';
```

아래 표는 주요 컬럼 통계항목에 대한 설명이다.
|통계항목|설명
|:---|:---|
|NUM_DISTINCT|컬럼 값의 종류 개수(NDV, Number of Distinct Values).<br/>예를 들어, 성별 컬럼이면 2|
|DENSITY|'=' 조건으로 검색할 때의 선택도를 미리 구해 놓은 값.<br/>히스토그램이 없거나, 있더라도 100% 균일한 분포를 갖는다면,<br/>1 / NUM_DISTINCT 값과 일치|
|AVG_COL_LEN|컬럼 평균 길이(Bytes)|
|LOW_VALUE|최소 값|
|HIGH_VALUE|최대 값|
|NUM_NULLS|값이 NULL인 레코드 수|

### [컬럼 히스토그램]
'=' 조건에 대한 선택도는 1/NUM_DISTINCT 공식으로 구하거나 미리 구해 놓은 DENSITY 값을 이용하면 된다.
일반적인 컬럼에는 이 공식이 비교적 잘 들어맞지만, 데이터 분포가 균일하지 않은 컬럼에는 그렇지 못하다.
선택도를 잘못 구하면 데이터 액세스 비용을 잘못 산정하게 되고, 결국 최적이 아닌 실행계획으로 이어진다. 그래서 옵티마이저는 일반적인 컬럼 통계 외에 히스토그램을 추가로 활용한다.

아파트매물 테이블에서 '시도구분' 컬럼에 대한 히스토그램을 원형 그래프로 표현하였다고 가정하자. 히스토그램은 이처럼 컬럼 값별로 데이터 비중 또는 빈도를 미리 계산해 놓은 통계정보다.
실제 데이터를 읽어서 계산해 둔 값이므로 데이터 분포가 많이 변하지 않는 한 거의 정확하다.

오라클 12c에서 사용하는 히스토그램 유형으로는 다음 네 가지가 있다.

|히스토그램 유형|설명|
|:---|:---|
|도수분포<br/>(FREQUENCY)|값별로 빈도수 저장|
|높이균형<br/>(HEIGHT-BALANCED)|각 버킷의 높이가 동일하도록 데이터 분포 관리|
|상위도수분포<br/>(TOP-FREQUENCY)|많은 레코드를 가진 상위 n개 값에 대한 빈도수 저장 (12c 이상)|
|하이브리드<br/>(HYBRID)|도수분포와 높이균형 히스토그램의 특성 결합 (12c 이상)|

히스토그램을 수집하려면, 테이블 통계 수집할 때 아래와 같이 method_opt 파라미터를 지정하면 된다.
```
begin
  dbms_stats.gather_table_stats('scott', 'emp'
        , cascade=>false, method_opt=>'for columns ename size 10, deptno size 4');
end;
/

begin
   dbms_stats.gather_table_stats('scott', 'emp'
        , cascade=>false, method_opt=>'for all columns size 75');
end;
/

begin
   dbms_stats.gather_table_stats('scott', 'emp'
        , cascade=>false, method_opt=>'for all columns size auto');
end;
/
```
수집된 컬럼 히스토그램은 아래와 같이 조회할 수 있다. all_tab_histograms 뷰에서도 같은 정보를 확인할 수 있다.
```
select endpoint_alue, endpoint_number
from   all_histograms
where  owner = 'SCOTT'
and    table_name = 'EMP'
and    column_name = 'DEPTNO'
order by endpoint_value;

ENDPOINT_VALUE ENDPOINT_NUMBER
-------------- ---------------
            10               3
            20               8
            30              14
```
출력되는 결과값은 히스토그램 유형에 따라 해석하는 방법이 다르다. 지금 단계에서 중요하게 다룰 내용은 아니므로 자세한 설명은 생략한다.

### [4] 시스템 통계
시스템 통계는 애플리케이션 및 하드웨어 성능 특성을 측정한 것이며, 아래 항목들을 포함한다.
- CPU 속도
- 평균적인 Single Block I/O 속도
- 평균적인 Multiblock I/O 속도
- 평균적인 Multiblock I/O 개수
- I/O 서브시스템의 최대 처리량(Throughput)
- 병렬 Slave의 평균적인 처리량(Throughput)

과거에는 옵티마이저가 이들 항목을 고려하지 않았다. 옵티마이저 개발팀이 사용한 하드웨어 사양에 맞춰진 고정 상수값으로 처리한 셈이다.
그러다 보니 실제 오라클이 설치된 운영시스템 사양이 그보다 좋거나 나쁠 때 옵티마이저가 최적이 아닌 실행계획을 수립할 가능성이 생긴다.

시스테메 사양뿐만 아니라 애플리케이션 특성(OLTP, DW) 및 동시 트랜잭션 발생량에 따라서도 이들 성능 특성이 달라진다.
이에 오라클은 제품이 설치된 하드웨어 및 애플리케이션 특성을 반영함으로써 옵티마이저가 보다 합리적으로 작동할 수 있게 하려고 9i부터 시스템 통계 수집 기능을 도입하였다.

시스템 통계는 아래와 같이 sys.aux_stats$ 뷰에서 조회할 수 있다.
```
SQL> select sname, pname, pval1, pval2 from sys.aux_stats$;

SNAME            PNAME            PVAL1            PVAL2
---------------- ---------------- ---------------- ----------------
SYSSTATS_INFO    STATUS                            COMPLETED
SYSSTATS_INFO    DSTART                            06-25-2008 10:28
SYSSTATS_INFO    DSTOP                             06-25-2008 11:10
SYSSTATS_INFO    FLAGS                           1
SYSSTATS_INFO    CPUSPEEDNW             1189.15231
SYSSTATS_INFO    IOSEEKTIM                      10
SYSSTATS_INFO    IOTFRSPEED                   4096
SYSSTATS_INFO    SREADTIM                    5.253
SYSSTATS_INFO    MREADTIM                    3.122
SYSSTATS_INFO    CPUSPEED                     1149
SYSSTATS_INFO    MBRC                           15
SYSSTATS_INFO    MAXTHR                 1856370688
SYSSTATS_INFO    SLAVETHR                 21365760
```

<br/>

## (3) 비용 계산 원리
지면 관계상, 조인까지 포함한 모든 비용 계산 원리를 다 설명하지는 않겠다. 그런 세세한 내용까지 알아야 할 이유도 없다.
단, 옵티마이저가 통계정보를 어떤 식으로 활용하는지는 이해할 필요가 있으므로 단일 테이블을 인덱스로 액세스할 때의 비용 계산 원리를 간단히 살펴보자.

인덱스 키값을 모두 '=' 조건으로 검색할 때는 아래와 같이 인덱스 통계만으로도 쉽게 비용을 계산할 수 있다.

```
  비용 = BLEVEL                                -- 인덱스 수직적 탐색 비용
      + AVG_LEAF_BLOCKS_PER_KEY               -- 인덱스 수평적 탐색 비용
      + AVG_DATA_BLOCKS_PER_KEY               -- 테이블 랜덤 액세스 비용
```
인덱스 키값이 모두 '=' 조건이 아닐 때는 아래와 같이 컬럼 통계까지 활용한다.
```
  비용 = BLEVEL                                -- 인덱스 수직적 탐색 비용
      + LEAF_BLOCKS       ✕ 유효 인덱스 선택도    -- 인덱스 수평적 탐색 비용
      + CLUSTERING_FACTOR  ✕ 유효 테이블 선택도   -- 테이블 랜덤 액세스 비용
```
BLEVEL, LEAF_BLOCKS, CLUSTERING_FACTOR는 인덱스 통계에서 얻을 수 있고, 유효 인덱스 선택도와 유효 테이블 선택도는 컬럼 통계 및 히스토그램을 이용해 계산한다.

유효 인덱스 선택도란, 전체 인덱스 레코드 중 액세스 조건(스캔 범위를 결정하는 조건절)에 의해 선택될 것으로 예상되는 레코드 비중(%)을 의미한다.

유효 테이블 선택도란, 전체 인덱스 레코드 중 인덱스 컬럼에 대한 모든 조건절에 의해 선택될 것으로 예상되는 레코드 비중(%)을 의미한다. 이들 조건절에 의해 테이블 액세스 여부가 결정된다.

- #### [비용(Cost)의 정확한 의미]

  방금 설명한 비용 계산식은 'I/O 비용 모델' 기준이다. I/O 비용 모델을 사용할 때 실행계힉에 나타나는 Cost는 '예상 I/O Call 횟수'를 의미한다.

  반면, 최신 'CPU 비용 모델'에서 Cost는 Single Block I/O를 기준으로 한 상대적 시간을 표현한다.
  예를 들어, Cost가 100으로 표시된다면, '우리 시스템에서 Single Block I/O를 100번 하는 정도의 시간'으로 해석하면 된다. 상대적 시간개념이다.

  CPU 비용 모델을 개발한 이유는, 같은 실행계획으로 같은 양의 데이터를 읽어도 애플리케이션 및 하드웨어 성능 특성에 따라 절대 소요시간이 다를 수 있어서다.
  똑같이 I/O Call을 100번 해도 그것이 Single Block I/O냐 Multiblock I/O냐에 따라 속도가 다르고, 시스템마다 Single Block I/O와 Multiblock I/O 속도도 모두 다르다.